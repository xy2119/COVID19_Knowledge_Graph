# COVID19 Knowledge Representation and Search Engine
## ðŸ“Œ Submission to The 2021 Imperial College Data Science Challenge


In response to the COVID-19 pandemic, [2021 Data Science Institute Natural Language Processing Challenge](https://www.imperial.ac.uk/data-science/) issued a call to action for researchers to apply text mining approaches to keep up with the exponential growth of coronavirus literature. Our goal is to is to retrieve biomedical information and construct knowledge graphs based on semantic representation.

<!-- ABOUT THE PROJECT -->
<h2 id="about-the-project"> :pencil: About The Project</h2>

**Semantic Search** and **Named Entity Recognition** tasks were carried out by finetuning BioBERT. The extracted entities and relationships from the titles and abstracts of the literature eventually contributed to the building of the Covid 19 knowledge graph.

Semantic Search Engine Workflow     [`notebook`](./notebooks/Covid19_Search_Engine_BioBERT.ipynb)

![image](./images/semantic_engine.png)

Named Entity Recognition Workflow   [`notebook`](./notebooks/Covid19_NER_BioBERT.ipynb)
![image](./images/bio_ner.png)

## Dataset
[COVID-19 Open Research Dataset (CORD-19)](https://github.com/allenai/cord19) is a corpus of academic papers about COVID-19 and related coronavirus research. It's curated and maintained by the Semantic Scholar team at the Allen Institute for AI to support text mining and NLP research. 


## Prerequisites

[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) <br>

<!--This project is written in Python programming language. <br>-->
The following are the major open source packages utilised in this project:
* NLTK
* SpaCy
* Matplotlib
* Scikit-Learn
* Pytorch
* TensorFlow
* Keras


<h2 id="folder-structure"> Folder Structure</h2>

    code
      .  
      â”œâ”€â”€ notebooks                                                       
      â”‚    â”œâ”€â”€ Covid19_Search_Engine_BioBERT.ipynb                   
      â”‚    â”œâ”€â”€ Covid19_NER_BioBERT.ipynb   
      â”‚    â””â”€â”€ README.md    
      â”‚    
      â”‚
      â””â”€â”€ src
            â”œâ”€â”€ 0_parse_the_data                        # Part 0 : Parse the Data                                  
            â”‚   â”œâ”€â”€ parse_the_data.py                   # extract document titles and abstract, output > bio_titles.txt
            â”‚   â””â”€â”€ README.md    
            â”‚
            â”œâ”€â”€  1_tokenization                          # Part 1 : Tokenization    
            â”‚     â”œâ”€â”€ tokenization.py                    # tokenized the text by creating top100k token list, output > *_tokens.txt 
            â”‚     â”œâ”€â”€ tokenizer-bio.json                  
            â”‚     â”œâ”€â”€ split_tokens.txt
            â”‚     â”œâ”€â”€ bpe_tokens.txt
            â”‚     â”œâ”€â”€ nltk_tokens.txt
            â”‚     â”œâ”€â”€ scispacy_tokens.txt
            â”‚     â”œâ”€â”€ bert-base-uncased-vocab.txt
            â”‚     â””â”€â”€ README.md    
            â”‚
            â”œâ”€â”€ 2_word_representation                   # Part 2 : Build Word Representations             
            â”‚   â”œâ”€â”€ n-gram                              
            â”‚   â”‚   â”œâ”€â”€ n_gram.py                       # create word embedding through n-gram
            â”‚   â”‚   â”œâ”€â”€ n_gram_word2id.txt             
            â”‚   â”‚   â””â”€â”€ README.md 
            â”‚   â”‚
            â”‚   â””â”€â”€ skip_gram                           
            â”‚       â”œâ”€â”€ skip_gram.py                    # create word embedding through skip-gram
            â”‚       â”œâ”€â”€ new_w2v.model                   # skip-gram model
            â”‚       â””â”€â”€ README.md
            â”‚
            â”œâ”€â”€ 3_visualise_word_representation         # Part 3 : Explore the Word Representations                    
            â”‚   â”œâ”€â”€ t_sne.py                            # visualised embeddings through t-sne, output > t_sne.png
            â”‚   â”œâ”€â”€ bio_t_sne.py  
            â”‚   â”œâ”€â”€ co-occurrence.py                    # find entities that co-occur with Covid 19, output > co-occurrence.csv
            â”‚   â”œâ”€â”€ co-occurrence.csv                   # co-occurrence output 
            â”‚   â”œâ”€â”€ semantic_sim.py                     # find entities that semantically similar with Covid 19, output > semantic_sim.txt
            â”‚   â”œâ”€â”€ semantic_sim.txt
            â”‚   â”œâ”€â”€ bio_dict                            # dict of biomedical entities for mapping
            â”‚   â””â”€â”€ README.md    
            . 



## ðŸŽ¯ RoadMap

Follow [end-to-end notebooks](./notebooks) in Google Colab 

`./src` is the submission source code in Pytorch


## Future Work
Create Neo4j knowledge graphs for coronavirus literatures.

## Contributing
If you have any questions or suggestions towards this repository, feel free to contact me at xy2119@ic.ac.uk.

Any kind of enhancement or contribution is welcomed!
